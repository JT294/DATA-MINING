{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/SAI990323/BIGRec.git\n"
      ],
      "metadata": {
        "id": "ZrxvhdGWUxgT",
        "outputId": "955e12f0-fc5a-4ade-d150-a66466192a45",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'BIGRec'...\n",
            "remote: Enumerating objects: 116, done.\u001b[K\n",
            "remote: Counting objects: 100% (116/116), done.\u001b[K\n",
            "remote: Compressing objects: 100% (76/76), done.\u001b[K\n",
            "remote: Total 116 (delta 57), reused 77 (delta 34), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (116/116), 1.97 MiB | 16.93 MiB/s, done.\n",
            "Resolving deltas: 100% (57/57), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd BIGRec"
      ],
      "metadata": {
        "id": "PGOJQuuhU2SF",
        "outputId": "03d80fba-3a0c-495e-ac6a-829a1b733026",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/BIGRec\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt\n"
      ],
      "metadata": {
        "id": "HeOmPn8hVEg8",
        "outputId": "bb72241a-94fb-4a45-87cb-3bfee711c2d2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/huggingface/peft.git (from -r requirements.txt (line 10))\n",
            "  Cloning https://github.com/huggingface/peft.git to /tmp/pip-req-build-6s_sl1yy\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/peft.git /tmp/pip-req-build-6s_sl1yy\n",
            "  Resolved https://github.com/huggingface/peft.git to commit 2f063e6342235842a00bca150a98854b89117986\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 1)) (1.3.0)\n",
            "Collecting appdirs (from -r requirements.txt (line 2))\n",
            "  Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting loralib (from -r requirements.txt (line 3))\n",
            "  Downloading loralib-0.1.2-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting bitsandbytes==0.37.2 (from -r requirements.txt (line 4))\n",
            "  Downloading bitsandbytes-0.37.2-py3-none-any.whl.metadata (7.7 kB)\n",
            "Collecting peft==0.3.0 (from -r requirements.txt (line 5))\n",
            "  Downloading peft-0.3.0-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting black (from -r requirements.txt (line 6))\n",
            "  Downloading black-25.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl.metadata (81 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting datasets (from -r requirements.txt (line 8))\n",
            "  Downloading datasets-3.3.2-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting fire (from -r requirements.txt (line 9))\n",
            "  Downloading fire-0.7.0.tar.gz (87 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: Cannot install peft 0.14.1.dev0 (from git+https://github.com/huggingface/peft.git) and peft==0.3.0 because these package versions have conflicting dependencies.\u001b[0m\u001b[31m\n",
            "\u001b[0m\n",
            "The conflict is caused by:\n",
            "    The user requested peft==0.3.0\n",
            "    The user requested peft 0.14.1.dev0 (from git+https://github.com/huggingface/peft.git)\n",
            "\n",
            "To fix this you could try to:\n",
            "1. loosen the range of package versions you've specified\n",
            "2. remove package versions to allow pip to attempt to solve the dependency conflict\n",
            "\n",
            "\u001b[31mERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/topics/dependency-resolution/#dealing-with-dependency-conflicts\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.getcwd()"
      ],
      "metadata": {
        "id": "Yesy91oIVcES",
        "outputId": "55011c30-272e-49cc-abf2-3a8bf143da51",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/BIGRec'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd data/movie/"
      ],
      "metadata": {
        "id": "66IKYiTNVsu3",
        "outputId": "a61e341a-6392-43b7-89c0-8a4f16a7cb08",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/BIGRec/data/movie\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "mp-XhbiuUbZe"
      },
      "outputs": [],
      "source": [
        "f = open('ratings.dat', 'r')\n",
        "data = f.readlines()\n",
        "f = open('movies.dat', 'r', encoding='ISO-8859-1')\n",
        "movies = f.readlines()\n",
        "movie_names = [_.split('::')[1] for _ in movies]\n",
        "movie_ids = [_.split('::')[0] for _ in movies]\n",
        "movie_dict = dict(zip(movie_ids, movie_names))\n",
        "id_mapping = dict(zip(movie_ids, range(len(movie_ids))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "i89NlpazUbZf"
      },
      "outputs": [],
      "source": [
        "interaction_dicts = dict()\n",
        "for line in data:\n",
        "    user_id, movie_id, rating, timestamp = line.split('::')\n",
        "    if user_id not in interaction_dicts:\n",
        "        interaction_dicts[user_id] = {\n",
        "            'movie_id': [],\n",
        "            'rating': [],\n",
        "            'timestamp': [],\n",
        "            'movie_title': [],\n",
        "        }\n",
        "    interaction_dicts[user_id]['movie_id'].append(movie_id)\n",
        "    interaction_dicts[user_id]['rating'].append(int(float(rating) > 3.0))\n",
        "    interaction_dicts[user_id]['timestamp'].append(timestamp)\n",
        "    interaction_dicts[user_id]['movie_title'].append(movie_dict[movie_id])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "iCmRcnUuUbZf"
      },
      "outputs": [],
      "source": [
        "with open('all.csv', 'w') as f:\n",
        "    import csv\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerow(['user_id', 'item_id', 'rating', 'timestamp', 'item_title'])\n",
        "    for user_id, user_dict in interaction_dicts.items():\n",
        "        writer.writerow([user_id, user_dict['movie_id'], user_dict['rating'], user_dict['timestamp'], user_dict['movie_title']])\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "73U_bbDcUbZg",
        "outputId": "b58745d7-5aaa-49d0-bf53-be4c9f734592",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "683268\n"
          ]
        }
      ],
      "source": [
        "sequential_interaction_list = []\n",
        "seq_len = 10\n",
        "for user_id in interaction_dicts:\n",
        "    temp = zip(interaction_dicts[user_id]['movie_id'], interaction_dicts[user_id]['rating'], interaction_dicts[user_id]['timestamp'], interaction_dicts[user_id]['movie_title'])\n",
        "    temp = sorted(temp, key=lambda x: int(x[2]))\n",
        "    result = zip(*temp)\n",
        "    interaction_dicts[user_id]['movie_id'], interaction_dicts[user_id]['rating'], interaction_dicts[user_id]['timestamp'], interaction_dicts[user_id]['movie_title'] = [list(_) for _ in result]\n",
        "    for i in range(10, len(interaction_dicts[user_id]['movie_id'])):\n",
        "        sequential_interaction_list.append(\n",
        "            [user_id, interaction_dicts[user_id]['movie_title'][i - seq_len: i],interaction_dicts[user_id]['movie_id'][i-seq_len:i], interaction_dicts[user_id]['rating'][i-seq_len:i], interaction_dicts[user_id]['movie_id'][i], interaction_dicts[user_id]['rating'][i], interaction_dicts[user_id]['timestamp'][i].strip('\\n')]\n",
        "        )\n",
        "print(len(sequential_interaction_list))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "KY-i2ABiUbZg"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "sequential_interaction_list = sorted(sequential_interaction_list, key=lambda x: int(x[-1]))\n",
        "with open('./train.csv', 'w') as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerow(['user_id', 'history_movie_title', 'history_movie_id', 'history_rating', 'movie_id', 'rating', 'timestamp'])\n",
        "    writer.writerows(sequential_interaction_list[:int(len(sequential_interaction_list)*0.8)])\n",
        "with open('./valid.csv', 'w') as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerow(['user_id', 'history_movie_title', 'history_movie_id', 'history_rating', 'movie_id', 'rating', 'timestamp'])\n",
        "    writer.writerows(sequential_interaction_list[int(len(sequential_interaction_list)*0.8):int(len(sequential_interaction_list)*0.9)])\n",
        "with open('./test.csv', 'w') as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerow(['user_id', 'history_movie_title', 'history_movie_id', 'history_rating', 'movie_id', 'rating', 'timestamp'])\n",
        "    writer.writerows(sequential_interaction_list[int(len(sequential_interaction_list)*0.9):])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "gW8Zld95UbZg"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import random\n",
        "import numpy as np\n",
        "def csv_to_json(input_path, output_path, sample=False):\n",
        "    data = pd.read_csv(input_path)\n",
        "    if sample:\n",
        "        data = data.sample(n=5000, random_state=42).reset_index(drop=True)\n",
        "        data.to_csv(output_path[:-5] + \".csv\", index=False)\n",
        "    json_list = []\n",
        "    for index, row in data.iterrows():\n",
        "        row['history_movie_id'] = eval(row['history_movie_id'])\n",
        "        row['history_movie_title'] = eval(row['history_movie_title'])\n",
        "        L = len(row['history_movie_id'])\n",
        "        history = \"The user has watched the following movies before:\"\n",
        "        for i in range(L):\n",
        "            if i == 0:\n",
        "                history += \"\\\"\" + row['history_movie_title'][i] + \"\\\"\"\n",
        "            else:\n",
        "                history += \", \\\"\" + row['history_movie_title'][i] + \"\\\"\"\n",
        "        target_movie_name = \"\\\"\" + movie_dict[str(row['movie_id'])] + \"\\\"\"\n",
        "        json_list.append({\n",
        "            \"instruction\": \"Given a list of movies the user has watched before, please recommend a new movie that the user likes to the user.\",\n",
        "            \"input\": f\"{history}\\n \",\n",
        "            \"output\": target_movie_name,\n",
        "        })\n",
        "\n",
        "    with open(output_path, 'w') as f:\n",
        "        json.dump(json_list, f, indent=4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "uqBRafToUbZh"
      },
      "outputs": [],
      "source": [
        "csv_to_json('./train.csv', './train.json')\n",
        "csv_to_json('./valid.csv', './valid.json')\n",
        "csv_to_json('./test.csv', './test.json')\n",
        "csv_to_json('./valid.csv', './valid_5000.json', sample=True)\n",
        "csv_to_json('./test.csv', './test_5000.json', sample=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd ..\n",
        "os.getcwd()"
      ],
      "metadata": {
        "id": "baGv8bKyYFXS",
        "outputId": "02433a41-6605-49eb-b15c-81b57e5b6d5a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/BIGRec\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/BIGRec'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 设置超参数\n",
        "seeds = [0, 1, 2]\n",
        "learning_rates = [1e-4]\n",
        "dropouts = [0.05]\n",
        "samples = [1024]\n",
        "\n",
        "# 循环运行模型\n",
        "for seed in seeds:\n",
        "    for lr in learning_rates:\n",
        "        for dropout in dropouts:\n",
        "            for sample in samples:\n",
        "                print(f\"lr: {lr}, dropout: {dropout}, seed: {seed},\")\n",
        "                # 构建命令\n",
        "                command = f\"CUDA_VISIBLE_DEVICES=$1 python train.py \\\n",
        "                    --base_model YOUR_LLAMA_PATH/ \\\n",
        "                    --train_data_path '[\\\"./data/movie/train.json\\\"]'   \\\n",
        "                    --val_data_path '[\\\"./data/movie/valid_5000.json\\\"]' \\\n",
        "                    --output_dir /model/movie/{seed}_{sample} \\\n",
        "                    --batch_size 128 \\\n",
        "                    --micro_batch_size 4 \\\n",
        "                    --num_epochs 50 \\\n",
        "                    --learning_rate {lr} \\\n",
        "                    --cutoff_len 512 \\\n",
        "                    --lora_r 8 \\\n",
        "                    --lora_alpha 16\\\n",
        "                    --lora_dropout {dropout} \\\n",
        "                    --lora_target_modules '[q_proj,v_proj]' \\\n",
        "                    --train_on_inputs \\\n",
        "                    --group_by_length \\\n",
        "                    --resume_from_checkpoint 'XXX' \\\n",
        "                    --seed {seed} \\\n",
        "                    --sample {sample}\"\n",
        "                # 执行命令\n",
        "                os.system(command)\n"
      ],
      "metadata": {
        "id": "nGPkVdLgYDdw",
        "outputId": "be58e7da-51f2-42ba-acd7-e2ad69bbd454",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lr: 0.0001, dropout: 0.05, seed: 0,\n",
            "lr: 0.0001, dropout: 0.05, seed: 1,\n",
            "lr: 0.0001, dropout: 0.05, seed: 2,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fire"
      ],
      "metadata": {
        "id": "pqoUYClQYyiL",
        "outputId": "fbea27b8-1183-4d53-a1e7-113bcf959bfd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fire\n",
            "  Using cached fire-0.7.0.tar.gz (87 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.11/dist-packages (from fire) (2.5.0)\n",
            "Building wheels for collected packages: fire\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.7.0-py3-none-any.whl size=114249 sha256=3bca929025ca1cbb91b64fd604d8aaba0c9bd982f6f5d1e942cd8abb55b4a45b\n",
            "  Stored in directory: /root/.cache/pip/wheels/46/54/24/1624fd5b8674eb1188623f7e8e17cdf7c0f6c24b609dfb8a89\n",
            "Successfully built fire\n",
            "Installing collected packages: fire\n",
            "Successfully installed fire-0.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "\n",
        "def run_command(command):\n",
        "    result = subprocess.run(command, shell=True, text=True, capture_output=True)\n",
        "    print(\"Status:\", result.returncode)  # 打印返回状态码\n",
        "    print(\"Output:\", result.stdout)      # 打印标准输出\n",
        "    if result.stderr:                    # 如果有标准错误输出，也打印它\n",
        "        print(\"Errors:\", result.stderr)\n",
        "\n",
        "# 示例命令\n",
        "command = f\"CUDA_VISIBLE_DEVICES=$1 python train.py \\\n",
        "    --base_model YOUR_LLAMA_PATH/ \\\n",
        "    --train_data_path '[\\\"./data/movie/train.json\\\"]'   \\\n",
        "    --val_data_path '[\\\"./data/movie/valid_5000.json\\\"]' \\\n",
        "    --output_dir /model/movie/{seed}_{sample} \\\n",
        "    --batch_size 128 \\\n",
        "    --micro_batch_size 4 \\\n",
        "    --num_epochs 50 \\\n",
        "    --learning_rate {lr} \\\n",
        "    --cutoff_len 512 \\\n",
        "    --lora_r 8 \\\n",
        "    --lora_alpha 16\\\n",
        "    --lora_dropout {dropout} \\\n",
        "    --lora_target_modules '[q_proj,v_proj]' \\\n",
        "    --train_on_inputs \\\n",
        "    --group_by_length \\\n",
        "    --resume_from_checkpoint 'XXX' \\\n",
        "    --seed {seed} \\\n",
        "    --sample {sample}\"\n",
        "run_command(command)"
      ],
      "metadata": {
        "id": "yzLeQ_nTYlcg",
        "outputId": "11554480-ebbc-4695-98ea-77abcc5a1876",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Status: 1\n",
            "Output: \n",
            "Errors: Traceback (most recent call last):\n",
            "  File \"/content/BIGRec/train.py\", line 10, in <module>\n",
            "    from datasets import load_dataset, concatenate_datasets\n",
            "ModuleNotFoundError: No module named 'datasets'\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "alpaca_lora",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "59fa845e12d05d721e6f4368480cbf49d04f4a649a02e83c3e47bffdee3cc61a"
      }
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}